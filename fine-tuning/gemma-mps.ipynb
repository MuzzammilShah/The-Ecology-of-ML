{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🌟 **UPDATED BY [Muhammed Shah](https://muhammedshah.com) to support running this on Apple Silicon** | **ORIGINAL AUTHOR OF THIS NOTEBOOK:** [Adithya SK](https://adithyask.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct Fine-tuning [Gemma](https://blog.google/technology/developers/gemma-open-models/) using qLora and Supervise Finetuning (MPS - Apple Silicon)\n",
    "\n",
    "This is a comprehensive notebook and tutorial on how to fine tune the `gemma-7b-it` Model on Apple Silicon using MPS (Metal Performance Shaders) - the Apple equivalent of NVIDIA CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPS vs CUDA: Key Differences for Apple Silicon\n",
    "\n",
    "**MPS (Metal Performance Shaders)** is Apple's equivalent to NVIDIA's CUDA for GPU acceleration on Apple Silicon Macs. Here are the key differences when running this notebook:\n",
    "\n",
    "#### Hardware Requirements:\n",
    "- **CUDA Version**: Requires NVIDIA GPU (RTX 3090, A100, etc.)\n",
    "- **MPS Version**: Requires Apple Silicon Mac (M1, M2, M3, M4) with macOS 12.3+\n",
    "\n",
    "#### Memory Considerations:\n",
    "- **CUDA**: Separate GPU VRAM (e.g., 24GB on RTX 3090)\n",
    "- **MPS**: Unified memory shared between CPU and GPU (16GB, 32GB, 64GB+ recommended)\n",
    "\n",
    "#### Quantization Support:\n",
    "- **CUDA**: Full support for 4-bit and 8-bit quantization with BitsAndBytes\n",
    "- **MPS**: Limited quantization support; may fall back to full precision or 8-bit\n",
    "\n",
    "#### Performance Notes:\n",
    "- **CUDA**: Generally faster for training large models\n",
    "- **MPS**: Excellent performance with lower power consumption, especially for inference\n",
    "\n",
    "This notebook has been adapted to automatically detect and use the appropriate backend for your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:\n",
    "\n",
    "1. **Apple Silicon Mac**: This notebook is designed to run on Apple Silicon Macs (M1, M2, M3, M4) using MPS (Metal Performance Shaders) - Apple's equivalent to NVIDIA CUDA. [gemma-2b](https://huggingface.co/google/gemma-2b) can be fine-tuned on most Apple Silicon Macs with 16GB+ unified memory, while [gemma-7b](https://huggingface.co/google/gemma-7b) requires Macs with 32GB+ unified memory.\n",
    "2. **Python Packages**: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:\n",
    "\n",
    "Let's begin by checking if MPS is available on your Apple Silicon Mac:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "System: Darwin arm64\n",
      "MPS available: True\n",
      "MPS built: True\n",
      "✅ MPS is available! You can use Apple Silicon GPU acceleration.\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Metal Performance Shaders) is available - Apple Silicon equivalent of CUDA\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"System: {platform.system()} {platform.machine()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS is available! You can use Apple Silicon GPU acceleration.\")\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    print(\"❌ MPS not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Model loading\n",
    "We'll load the model using QLoRA quantization to reduce memory usage. On Apple Silicon MPS, we'll use the most compatible quantization method or fall back to full precision if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "✅ Packages installed. Note: Some quantization features may have limited support on MPS (Apple Silicon)\n",
      "✅ Packages installed. Note: Some quantization features may have limited support on MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages - MPS (Apple Silicon) compatible versions\n",
    "!pip3 install -q -U bitsandbytes==0.42.0  # May have limited MPS support\n",
    "!pip3 install -q -U peft==0.8.2\n",
    "!pip3 install -q -U trl==0.7.10\n",
    "!pip3 install -q -U accelerate==0.27.1\n",
    "!pip3 install -q -U datasets==2.17.0\n",
    "!pip3 install -q -U transformers==4.38.0\n",
    "\n",
    "# For Apple Silicon, ensure you have the latest PyTorch with MPS support\n",
    "# If you encounter issues, you may need to install PyTorch nightly:\n",
    "# !pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "\n",
    "print(\"✅ Packages installed. Note: Some quantization features may have limited support on MPS (Apple Silicon)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muzzmac/Desktop/projects/The-Ecology-of-ML/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using 8-bit quantization for MPS compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Check device availability\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Note: 4-bit quantization with BitsAndBytes may have limited support on MPS\n",
    "# For Apple Silicon, we'll use a more compatible configuration\n",
    "if device == \"mps\":\n",
    "    # For MPS (Apple Silicon), use 8-bit quantization or no quantization\n",
    "    # 4-bit quantization support on MPS is experimental\n",
    "    try:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,  # Use 8-bit instead of 4-bit for better MPS compatibility\n",
    "        )\n",
    "        print(\"Using 8-bit quantization for MPS compatibility\")\n",
    "    except:\n",
    "        # Fallback: no quantization on MPS if BitsAndBytes doesn't support it\n",
    "        bnb_config = None\n",
    "        print(\"BitsAndBytes quantization not supported on MPS, using full precision\")\n",
    "else:\n",
    "    # Original CUDA configuration for reference\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we specify the model ID and then we load it with our previously defined quantization configuration.Now we specify the model ID and then we load it with our previously defined quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are using google colab\n",
    "\n",
    "# import os\n",
    "# from google.colab import userdata\n",
    "# os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb1443f307f4a2c8ef2b8a44dcab640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Method 1: if you are using Jupyter Notebook or any other IDE\n",
    "# !pip3 install ipywidgets\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Method 2 (If method 1 doesn't work):\n",
    "\n",
    "# Run the below command if its the first time you are running this notebook\n",
    "# !pip3 install python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the token from environment variables\n",
    "hf_token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"Successfully logged in to Hugging Face!\")\n",
    "else:\n",
    "    print(\"Warning: HUGGINGFACE_HUB_TOKEN not found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"google/gemma-7b-it\"\n",
    "# # model_id = \"google/gemma-7b\"\n",
    "# # model_id = \"google/gemma-2b-it\"\n",
    "# # model_id = \"google/gemma-2b\"\n",
    "\n",
    "# # Load model with MPS-compatible settings\n",
    "# if device == \"mps\" and bnb_config is not None:\n",
    "#     # Load with quantization if supported\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id, \n",
    "#         quantization_config=bnb_config, \n",
    "#         device_map=\"auto\",  # Let transformers handle device mapping\n",
    "#         torch_dtype=torch.float16  # Use float16 for memory efficiency on Apple Silicon\n",
    "#     )\n",
    "# elif device == \"mps\":\n",
    "#     # Load without quantization for MPS\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id,\n",
    "#         torch_dtype=torch.float16,  # Use float16 for memory efficiency\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "# else:\n",
    "#     # Original CUDA configuration\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "\n",
    "# print(f\"Model loaded on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logged in to Hugging Face\n",
      "🔄 Loading model: google/gemma-2-2b-it\n",
      "This may take several minutes for first-time download...\n",
      "❌ Error loading model: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like google/gemma-2-2b-it is not the path to a directory containing a file named config.json.\n",
      "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n",
      "\n",
      "Troubleshooting steps:\n",
      "1. Make sure you accepted the Gemma license at: https://huggingface.co/google/gemma-7b-it\n",
      "2. Check your internet connection\n",
      "3. Verify your Hugging Face token has the right permissions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load environment variables and login\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"✅ Logged in to Hugging Face\")\n",
    "else:\n",
    "    raise ValueError(\"❌ HUGGINGFACE_HUB_TOKEN not found in .env file\")\n",
    "\n",
    "# Model configuration\n",
    "# model_id = \"google/gemma-7b-it\"\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "try:\n",
    "    print(f\"🔄 Loading model: {model_id}\")\n",
    "    print(\"This may take several minutes for first-time download...\")\n",
    "    \n",
    "    # Load model with MPS-compatible settings\n",
    "    if device == \"mps\" and bnb_config is not None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            quantization_config=bnb_config, \n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            token=hf_token  # Explicit token passing\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            token=hf_token  # Explicit token passing\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            quantization_config=bnb_config, \n",
    "            device_map={\"\":0},\n",
    "            token=hf_token  # Explicit token passing\n",
    "        )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id, \n",
    "        add_eos_token=True,\n",
    "        token=hf_token  # Explicit token passing\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Model loaded on device: {next(model.parameters()).device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Make sure you accepted the Gemma license at: https://huggingface.co/google/gemma-7b-it\")\n",
    "    print(\"2. Check your internet connection\")\n",
    "    print(\"3. Verify your Hugging Face token has the right permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are cached in: /Users/muzzmac/.cache/huggingface/hub/\n",
      "Available space: 386.3 GB\n"
     ]
    }
   ],
   "source": [
    "# To check cache location, where our models are downloaded\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "# Show cache directory\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/\")\n",
    "print(f\"Models are cached in: {cache_dir}\")\n",
    "\n",
    "# Check available space (models can be several GB)\n",
    "import shutil\n",
    "free_space = shutil.disk_usage(cache_dir).free / (1024**3)  # GB\n",
    "print(f\"Available space: {free_space:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "    # Use MPS if available, otherwise CPU - Apple Silicon equivalent of \"cuda:0\"\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    <start_of_turn>user\n",
    "    Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "    {query}\n",
    "    <end_of_turn>\\n<start_of_turn>model\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    prompt = prompt_template.format(query=query)\n",
    "\n",
    "    encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    model_inputs = encodeds.to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs, \n",
    "        max_new_tokens=1000, \n",
    "        do_sample=True, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Load dataset for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Load the Dataset\n",
    "\n",
    "For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.\n",
    "\n",
    "We will be using this [dataset](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style) which is curated by [TokenBender (e/xperiments)](https://twitter.com/4evaBehindSOTA) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n",
    "  \"input\": \"[1, 2, 3, 4, 5]\",\n",
    "  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :\n",
    "1. the function generate_prompt : take the instruction and output and generate a prompt\n",
    "2. shuffle the dataset\n",
    "3. tokenizer the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Dataset\n",
    "\n",
    "Now, let's format the dataset in the required [gemma instruction formate](https://huggingface.co/google/gemma-7b-it).\n",
    "\n",
    "> Many tutorials and blogs skip over this part, but I feel this is a really important step.\n",
    "\n",
    "```\n",
    "<start_of_turn>user What is your favorite condiment? <end_of_turn>\n",
    "<start_of_turn>model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!<end_of_turn>\n",
    "```\n",
    "\n",
    "You can use the following code to process your dataset and create a JSONL file in the correct format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n",
    "\n",
    "    :param data_point: dict: Data point\n",
    "    :return: dict: tokenzed prompt\n",
    "    \"\"\"\n",
    "    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n",
    "               'appropriately completes the request.\\n\\n'\n",
    "    # Samples with additional context into.\n",
    "    if data_point['input']:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n",
    "    # Without\n",
    "    else:\n",
    "        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n",
    "    return text\n",
    "\n",
    "# add the \"prompt\" column in the dataset\n",
    "text_column = [generate_prompt(data_point) for data_point in dataset]\n",
    "dataset = dataset.add_column(\"prompt\", text_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to tokenize our data so the model can understand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into 90% for training and 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Formatting, We should get something like this\n",
    "\n",
    "```json\n",
    "{\n",
    "\"text\":\"<start_of_turn>user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] <end_of_turn>\n",
    "<start_of_turn>model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum <end_of_turn>\",\n",
    "\"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n",
    "\"input\":\"[1, 2, 3, 4, 5]\",\n",
    "\"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,\n",
    " sequence: sum += num return sum\",\n",
    "\"prompt\":\"<start_of_turn>user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] <end_of_turn>\n",
    "<start_of_turn>model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum <end_of_turn>\"\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "While using SFT (**[Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)**) for fine-tuning, we will be only passing in the “text” column of the dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Apply Lora  \n",
    "Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency on Apple Silicon\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare model for training - works on both MPS and CUDA\n",
    "if bnb_config is not None:\n",
    "    # If using quantization\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"Model prepared for quantized training (Apple Silicon MPS compatible)\")\n",
    "else:\n",
    "    # If not using quantization, ensure model is in training mode\n",
    "    model.train()\n",
    "    print(\"Model prepared for full-precision training on MPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    # For MPS (Apple Silicon), we need to handle different layer types\n",
    "    if device == \"mps\" and bnb_config is not None:\n",
    "        # If using 8-bit quantization on MPS\n",
    "        cls = bnb.nn.Linear8bitLt\n",
    "    elif bnb_config is not None:\n",
    "        # Original CUDA 4-bit quantization\n",
    "        cls = bnb.nn.Linear4bit\n",
    "    else:\n",
    "        # No quantization - use standard PyTorch Linear layers (MPS compatible)\n",
    "        cls = torch.nn.Linear\n",
    "    \n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "    \n",
    "    print(f\"Targeting layer type: {cls} (MPS compatible: {device == 'mps'})\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Run the training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the training arguments:\n",
    "* for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# trainer = transformers.Trainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_data,\n",
    "#     eval_dataset=test_data,\n",
    "#     args=transformers.TrainingArguments(\n",
    "#         per_device_train_batch_size=1,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         warmup_steps=0.03,\n",
    "#         max_steps=100,\n",
    "#         learning_rate=2e-4,\n",
    "#         fp16=True,\n",
    "#         logging_steps=1,\n",
    "#         output_dir=\"outputs_mistral_b_finance_finetuned_test\",\n",
    "#         optim=\"paged_adamw_8bit\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#     ),\n",
    "#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning with qLora and Supervised Fine-Tuning\n",
    "\n",
    "We're ready to fine-tune our model using qLora. For this tutorial, we'll use the `SFTTrainer` from the `trl` library for supervised fine-tuning. Ensure that you've installed the `trl` library as mentioned in the prerequisites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning with qLora and Supervised Fine-Tuning on Apple Silicon MPS\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Clear cache - MPS equivalent of torch.cuda.empty_cache()\n",
    "if device == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"MPS cache cleared (Apple Silicon equivalent of CUDA cache clear)\")\n",
    "else:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Configure training arguments for MPS (Apple Silicon)\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,  # Keep small for memory efficiency on Apple Silicon\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=0.03,\n",
    "    max_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    # Note: fp16 may not be fully supported on MPS, use bf16 if available\n",
    "    fp16=False if device == \"mps\" else True,\n",
    "    bf16=True if device == \"mps\" and torch.backends.mps.is_available() else False,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    # Use standard AdamW for MPS compatibility instead of paged_adamw_8bit\n",
    "    optim=\"adamw_torch\" if device == \"mps\" else \"paged_adamw_8bit\",\n",
    "    save_strategy=\"epoch\",\n",
    "    dataloader_pin_memory=False,  # Disable pin memory for MPS\n",
    "    dataloader_num_workers=0,     # Use single worker for MPS stability\n",
    ")\n",
    "\n",
    "print(f\"Training configured for device: {device}\")\n",
    "print(f\"Using optimizer: {training_args.optim}\")\n",
    "print(f\"Using precision: {'bf16' if training_args.bf16 else 'fp16' if training_args.fp16 else 'fp32'}\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Share adapters on the 🤗 Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"gemma-Code-Instruct-Finetune-test\" #Name of the model you will be pushing to huggingface model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model for merging - MPS compatible\n",
    "if device == \"mps\":\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float16,  # Use float16 for Apple Silicon\n",
    "        device_map=\"auto\",  # Let transformers handle MPS device mapping\n",
    "    )\n",
    "else:\n",
    "    # Original CUDA configuration\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map={\"\": 0},\n",
    "    )\n",
    "\n",
    "# Merge the LoRA adapter with base model (works on both MPS and CUDA)\n",
    "merged_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Model merged and saved successfully on {device} (Apple Silicon MPS compatible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the model and tokenizer to the Hugging Face Model Hub\n",
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPS Troubleshooting for Apple Silicon\n",
    "\n",
    "If you encounter issues while running on Apple Silicon, try these solutions:\n",
    "\n",
    "### Common MPS Issues and Solutions:\n",
    "\n",
    "1. **Memory Issues**: \n",
    "   - Reduce `per_device_train_batch_size` to 1\n",
    "   - Increase `gradient_accumulation_steps` to maintain effective batch size\n",
    "   - Use smaller model variant (gemma-2b instead of gemma-7b)\n",
    "\n",
    "2. **Quantization Errors**:\n",
    "   - If BitsAndBytes fails, the notebook will automatically fall back to full precision\n",
    "   - You can manually disable quantization by setting `bnb_config = None`\n",
    "\n",
    "3. **Training Instability**:\n",
    "   - MPS sometimes works better with `bf16=True` instead of `fp16=True`\n",
    "   - Try reducing learning rate to `1e-4` if training is unstable\n",
    "\n",
    "4. **Performance Optimization**:\n",
    "   - Ensure you're using the latest PyTorch version with MPS support\n",
    "   - Close other memory-intensive applications during training\n",
    "   - Use `torch.mps.empty_cache()` to clear MPS memory between operations\n",
    "\n",
    "### Memory Usage Guidelines:\n",
    "- **16GB Unified Memory**: Use gemma-2b model\n",
    "- **32GB+ Unified Memory**: Can handle gemma-7b model\n",
    "- **64GB+ Unified Memory**: Comfortable training with larger batch sizes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
